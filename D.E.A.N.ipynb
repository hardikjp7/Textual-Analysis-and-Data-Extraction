{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3534,"status":"ok","timestamp":1705481044310,"user":{"displayName":"Hardik Parmar","userId":"17058602810376678540"},"user_tz":-330},"id":"rWw1gYRulM53","outputId":"6048cd6f-60cb-4c18-d551-94dbf9c0d460"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/MyDrive/BlackCoffer_Task\n"]}],"source":["from google.colab import drive\n","\n","def mount_gdrive(gdrive_folder):\n","    # Mount Google Drive and set the mount point to the specified folder\n","    drive.mount('/gdrive')\n","\n","    # Change the current working directory to the project folder\n","    project_folder = f'/gdrive/{gdrive_folder}'\n","    %cd $project_folder\n","\n","# Specify your Google Drive folder here\n","your_gdrive_folder = 'MyDrive/---'\n","\n","# Mount Google Drive and set the current working directory\n","mount_gdrive(your_gdrive_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4241,"status":"ok","timestamp":1705481059902,"user":{"displayName":"Hardik Parmar","userId":"17058602810376678540"},"user_tz":-330},"id":"dW7IM5jSesNU","outputId":"6b96a3e7-2bfe-41d2-b41b-21f9a8daf0d5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31644,"status":"ok","timestamp":1705481557547,"user":{"displayName":"Hardik Parmar","userId":"17058602810376678540"},"user_tz":-330},"id":"Ja2m6o0Kg82q","outputId":"ef019192-2e9d-4463-cafa-28a9f4fe2697"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error getting response for blackassign0036: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n","Error getting response for blackassign0049: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"]}],"source":["import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","def get_page_content(url, url_id):\n","    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n","\n","    try:\n","        response = requests.get(url, headers=header)\n","        response.raise_for_status()  # Raise an HTTPError for bad responses\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error getting response for {url_id}: {e}\")\n","        return None\n","\n","    try:\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","    except Exception as e:\n","        print(f\"Error creating BeautifulSoup object for {url_id}: {e}\")\n","        return None\n","\n","    return soup\n","\n","def main():\n","    # Read the url file into the pandas object\n","    try:\n","        df = pd.read_excel('Input.xlsx')\n","    except Exception as e:\n","        print(f\"Error reading Excel file: {e}\")\n","        return\n","\n","    # Loop through each row in the df\n","    for index, row in df.iterrows():\n","        url = row['URL']\n","        url_id = row['URL_ID']\n","\n","        # Get page content\n","        soup = get_page_content(url, url_id)\n","        if soup is None:\n","            continue\n","\n","        # Find title\n","        try:\n","            title = soup.find('h1').get_text()\n","        except AttributeError as e:\n","            print(f\"Error getting title for {url_id}: {e}\")\n","            continue\n","\n","        # Find text\n","        article = \"\"\n","        try:\n","            for p in soup.find_all('p'):\n","                article += p.get_text()\n","        except Exception as e:\n","            print(f\"Error getting text for {url_id}: {e}\")\n","\n","        # Write title and text to the file\n","        file_name = f'/gdrive/MyDrive/BlackCoffer_Task/TitleText/{url_id}.txt'\n","        try:\n","            with open(file_name, 'w', encoding='utf-8') as file:\n","                file.write(title + '\\n' + article)\n","        except Exception as e:\n","            print(f\"Error writing to file {file_name}: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shR_yvHLhzUQ"},"outputs":[],"source":["# Directories\n","text_dir = \"/gdrive/MyDrive/---/TitleText\"\n","stopwords_dir = \"/gdrive/MyDrive/---/StopWords\"\n","sentment_dir = \"/gdrive/MyDrive/---/MasterDictionary\"\n","\n","# load all stop wors from the stopwords directory and store in the set variable\n","stop_words = set()\n","for files in os.listdir(stopwords_dir):\n","  with open(os.path.join(stopwords_dir,files),'r',encoding='ISO-8859-1') as f:\n","    stop_words.update(set(f.read().splitlines()))\n","\n","# load all text files  from the  directory and store in a list(docs)\n","docs = []\n","for text_file in os.listdir(text_dir):\n","  with open(os.path.join(text_dir,text_file),'r') as f:\n","    text = f.read()\n","#tokenize the given text file\n","    words = word_tokenize(text)\n","# remove the stop words from the tokens\n","    filtered_text = [word for word in words if word.lower() not in stop_words]\n","# add each filtered tokens of each file into a list\n","    docs.append(filtered_text)\n","\n","\n","\n","# store positive, Negative words from the directory\n","pos=set()\n","neg=set()\n","\n","for files in os.listdir(sentment_dir):\n","  if files =='positive-words.txt':\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      pos.update(f.read().splitlines())\n","  else:\n","    with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1') as f:\n","      neg.update(f.read().splitlines())\n","\n","# now collect the positive  and negative words from each file\n","# calculate the scores from the positive and negative words\n","positive_words = []\n","Negative_words =[]\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []\n","\n","#iterate through the list of docs\n","for i in range(len(docs)):\n","  positive_words.append([word for word in docs[i] if word.lower() in pos])\n","  Negative_words.append([word for word in docs[i] if word.lower() in neg])\n","  positive_score.append(len(positive_words[i]))\n","  negative_score.append(len(Negative_words[i]))\n","  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n","  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdiqezyJo_3K"},"outputs":[],"source":["import os\n","import re\n","from nltk.corpus import stopwords\n","\n","text_dir = \"/gdrive/MyDrive/---/TitleText\"\n","stopwords = set(stopwords.words('english'))\n","\n","def measure_complexity(file_path):\n","    with open(file_path, 'r') as f:\n","        text = f.read()\n","\n","    # Remove punctuations\n","    text = re.sub(r'[^\\w\\s.]', '', text)\n","\n","    # Split the text into sentences\n","    sentences = text.split('.')\n","\n","    # Total number of sentences in a file\n","    num_sentences = len(sentences)\n","\n","    # Total words in the file\n","    words = [word for word in text.split() if word.lower() not in stopwords]\n","    num_words = len(words)\n","\n","    # Complex words (syllable count greater than 2)\n","    complex_words = [word for word in words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]\n","\n","    # Syllable Count Per Word\n","    syllable_count = sum(sum(1 for letter in word if letter.lower() in 'aeiou') for word in words)\n","\n","    # Average sentence length\n","    avg_sentence_length = num_words / num_sentences\n","\n","    # Percentage of Complex words\n","    percentage_of_complex_words = len(complex_words) / num_words\n","\n","    # Fog Index\n","    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n","\n","    # Average syllable count per word\n","    avg_syllable_word_count = syllable_count / num_words\n","\n","    return avg_sentence_length, percentage_of_complex_words, fog_index, len(complex_words), avg_syllable_word_count\n","\n","# Measure complexity for each file in text_dir\n","avg_sentence_length = []\n","percentage_of_complex_words = []\n","fog_index = []\n","complex_word_count = []\n","avg_syllable_word_count = []\n","\n","for file_name in os.listdir(text_dir):\n","    file_path = os.path.join(text_dir, file_name)\n","    x, y, z, a, b = measure_complexity(file_path)\n","    avg_sentence_length.append(x)\n","    percentage_of_complex_words.append(y)\n","    fog_index.append(z)\n","    complex_word_count.append(a)\n","    avg_syllable_word_count.append(b)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAH4PXkLpVvI"},"outputs":[],"source":["import os\n","import re\n","from nltk.corpus import stopwords\n","\n","text_dir = \"/gdrive/MyDrive/---/TitleText\"\n","stopwords = set(stopwords.words('english'))\n","\n","def cleaned_words(file_path):\n","    with open(file_path, 'r') as f:\n","        text = f.read()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        words = [word for word in text.split() if word.lower() not in stopwords]\n","        total_word_length = sum(len(word) for word in words)\n","        average_word_length = total_word_length / len(words)\n","    return len(words), average_word_length\n","\n","def count_personal_pronouns(file_path):\n","    with open(file_path, 'r') as f:\n","        text = f.read()\n","        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","        count = sum(len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) for pronoun in personal_pronouns)\n","    return count\n","\n","# Word Count and Average Word Length\n","word_count = []\n","average_word_length = []\n","for file_name in os.listdir(text_dir):\n","    file_path = os.path.join(text_dir, file_name)\n","    x, y = cleaned_words(file_path)\n","    word_count.append(x)\n","    average_word_length.append(y)\n","\n","# Count Personal Pronouns\n","pp_count = []\n","for file_name in os.listdir(text_dir):\n","    file_path = os.path.join(text_dir, file_name)\n","    x = count_personal_pronouns(file_path)\n","    pp_count.append(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":694,"status":"ok","timestamp":1705482597056,"user":{"displayName":"Hardik Parmar","userId":"17058602810376678540"},"user_tz":-330},"id":"rSQD8Odhp6zp","outputId":"5304ab4a-74a8-451d-c7bd-e1f7b96fff1f"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-18-c702e75f6d38>:29: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n","  output_df.iloc[:, i + 2] = var\n"]}],"source":["import pandas as pd\n","\n","# Read the output data structure from the Excel file\n","output_df = pd.read_excel('Output Data Structure.xlsx')\n","\n","# URL_ID 36 and 49 do not exist (i.e., page does not exist and throws a 404 error)\n","# Drop these rows from the table\n","output_df.drop([36, 49], axis=0, inplace=True)\n","\n","# Define the required parameters\n","variables = [\n","    positive_score,\n","    negative_score,\n","    polarity_score,\n","    subjectivity_score,\n","    avg_sentence_length,\n","    percentage_of_complex_words,\n","    fog_index,\n","    avg_sentence_length,\n","    complex_word_count,\n","    word_count,\n","    avg_syllable_word_count,\n","    pp_count,\n","    average_word_length\n","]\n","\n","# Write the values to the dataframe\n","for i, var in enumerate(variables):\n","    output_df.iloc[:, i + 2] = var\n","\n","# Save the dataframe to a CSV file\n","output_df.to_csv('Output_Data.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEXOD6_oqxnA"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOwga7p8WFuuVI5J6O9ZYWj","mount_file_id":"1nFJJ9b0QEnf9xEQJ08o7hc6C6H-LvnlV","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
